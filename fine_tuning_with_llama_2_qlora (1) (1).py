# -*- coding: utf-8 -*-
"""fine-tuning-with-llama-2-qlora.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SqGrKtNjCKz5jSqQgM3RK1YU79cmg6PN
"""

import pandas as pd
import json
import os
from pprint import pprint
import bitsandbytes as bnb
import torch
import torch.nn as nn
import transformers
from datasets import load_dataset, Dataset
# from huggingface_hub import notebook_login

from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"

from huggingface_hub import login
login(token="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")

model = "meta-llama/Llama-2-7b-chat-hf"
MODEL_NAME = model

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    trust_remote_code=True,
    quantization_config=bnb_config
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

model = prepare_model_for_kbit_training(model)

import re
def get_num_layers(model):
    numbers = set()
    for name, _ in model.named_parameters():
        for number in re.findall(r'\d+', name):
            numbers.add(int(number))
    return max(numbers)

def get_last_layer_linears(model):
    names = []

    num_layers = get_num_layers(model)
    for name, module in model.named_modules():
        if str(num_layers) in name and not "encoder" in name:
            if isinstance(module, torch.nn.Linear):
                names.append(name)
    return names

config = LoraConfig(
    r=2,
    lora_alpha=32,
    target_modules=get_last_layer_linears(model),
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, config)
model.gradient_checkpointing_enable()

def count_parameters(model):
    num_params = 0
    for name, module in model.named_modules():
        if name in target_modules:
            num_params += sum(p.numel() for p in module.parameters())
    return num_params

def count_total_parameters(model):
    total_params = sum(p.numel() for p in model.parameters())
    return total_params

target_modules = get_last_layer_linears(model)
num_updating_params = count_parameters(model)
total_params = count_total_parameters(model)

print(f"Number of parameters being updated: {num_updating_params}")
print(f"Total number of parameters in the model: {total_params}")

df = pd.read_csv("/home/ritwik21557/IP_NOVEL_RECIPE/PROJECT_LLAMA2/training_data.csv")
df.columns = [str(q).strip() for q in df.columns]

# Select rows with indices 0 to 2000 and 10000 to 12000
df = pd.concat([df.iloc[0:2000], df.iloc[10000:12000], df.iloc[13000:15000]])
# df=df.tail(10)
print(df.head())
data = Dataset.from_pandas(df)

# print(df["ground_truth_input"].values[0])

# prompt = df["Question"].values[0] + ". Answer as briefly as possible: ".strip()
# prompt=f"<s> [INST] <<SYS>> You are a helpful assistant, who always provides empathetic responses to the user's query and helps them solve their mental problems. <</SYS>> {df["ground_truth_input"].values[0]} [/INST]"
# prompt

# generation_config = model.generation_config
# generation_config.max_new_tokens = 150
# generation_config.temperature = 0.5
# generation_config.top_p = 0.8
# generation_config.num_return_sequences = 1
# generation_config.pad_token_id = tokenizer.eos_token_id
# generation_config.eos_token_id = tokenizer.eos_token_id

# %%time
# device = "cuda"

# encoding = tokenizer(prompt, return_tensors="pt").to(device)
# with torch.no_grad():
#     outputs = model.generate(
#         input_ids = encoding.input_ids,
#         attention_mask = encoding.attention_mask,
#         generation_config = generation_config
#     )

# print(tokenizer.decode(outputs[0], skip_special_tokens=True))

def generate_prompt(data_point):
    ans=f"<s> [INST] <<SYS>> You are a helpful assistant, who always provides empathetic responses to the user's query and helps them solve their mental problems. <</SYS>> {data_point["ground_truth_input"]} [/INST] {data_point["ground_truth_output"]} </s>"
    # print(ans)
    return ans

def generate_and_tokenize_prompt(data_point):
    full_prompt = generate_prompt(data_point)
    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)
    return tokenized_full_prompt

data = data.shuffle().map(generate_and_tokenize_prompt)

import time
start_time = time.time()

training_args = transformers.TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=5,
    learning_rate=3e-4,
    fp16=True,
    output_dir="finetune_jeopardy",
    optim="paged_adamw_8bit",
    lr_scheduler_type="cosine",
    save_strategy='epoch',
    warmup_ratio=0.01,
    report_to="none"
)

trainer = transformers.Trainer(
    model=model,
    train_dataset=data,
    args=training_args,
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)
)
model.config.use_cache = False
trainer.train()
end_time = time.time()

# Calculate the time taken in seconds
time_taken = end_time - start_time

# Convert to minutes
time_taken_minutes = time_taken / 60

print(f"Time taken by the function: {time_taken_minutes:.2f} minutes")

model.save_pretrained("trained-model")

"""## Loading and using the model later

Now, we'll save the PEFT fine-tuned model, then load it and use it to generate some more answers.
"""

# bnb_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_use_double_quant=True,
#     bnb_4bit_quant_type="nf4",
#     bnb_4bit_compute_dtype=torch.bfloat16
# )

# PEFT_MODEL = "trained-model"

# config = PeftConfig.from_pretrained(PEFT_MODEL)

# model = AutoModelForCausalLM.from_pretrained(
#     config.base_model_name_or_path,
#     return_dict=True,
#     quantization_config=bnb_config,
#     device_map="auto",
#     trust_remote_code=True
# )

# tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)
# tokenizer.pad_token = tokenizer.eos_token

# model = PeftModel.from_pretrained(model, PEFT_MODEL)

# generation_config = model.generation_config
# generation_config.max_new_tokens = 250
# generation_config.temperature = 0.5
# generation_config.top_p = 0.8
# generation_config.num_return_sequences = 1
# generation_config.pad_token_id = tokenizer.eos_token_id
# generation_config.eos_token_id = tokenizer.eos_token_id

# import numpy as np

# %%time

# prompt=f"Given that a person has the following ingredients available, 'worcestershire sauce', 'mayonnaise', 'black pepper', 'beef broth', 'lettuce', 'onion', 'salt', 'milk', 'ketchup', 'sub', 'velveeta cheese', 'garlic clove', 'beef'. Provide the name of exactly one novel recipe along with the detailed instructions to make it from the ingredients mentioned above.".strip()

# device = "cuda"
# encoding = tokenizer(prompt, return_tensors="pt").to(device)
# with torch.inference_mode():
#   outputs = model.generate(
#       input_ids = encoding.input_ids,
#       attention_mask = encoding.attention_mask,
#       generation_config = generation_config
#   )

# print(tokenizer.decode(outputs[0], skip_special_tokens=True))