{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9903284,"sourceType":"datasetVersion","datasetId":6083864}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install python-telegram-bot==13.15\n!pip install -U bitsandbytes\n!pip install peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T11:07:55.222822Z","iopub.execute_input":"2024-11-19T11:07:55.223142Z","iopub.status.idle":"2024-11-19T11:08:30.166286Z","shell.execute_reply.started":"2024-11-19T11:07:55.223112Z","shell.execute_reply":"2024-11-19T11:08:30.165382Z"}},"outputs":[{"name":"stdout","text":"Collecting python-telegram-bot==13.15\n  Downloading python_telegram_bot-13.15-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from python-telegram-bot==13.15) (2024.8.30)\nCollecting tornado==6.1 (from python-telegram-bot==13.15)\n  Downloading tornado-6.1.tar.gz (497 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.4/497.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting APScheduler==3.6.3 (from python-telegram-bot==13.15)\n  Downloading APScheduler-3.6.3-py2.py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: pytz>=2018.6 in /opt/conda/lib/python3.10/site-packages (from python-telegram-bot==13.15) (2024.1)\nCollecting cachetools==4.2.2 (from python-telegram-bot==13.15)\n  Downloading cachetools-4.2.2-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: setuptools>=0.7 in /opt/conda/lib/python3.10/site-packages (from APScheduler==3.6.3->python-telegram-bot==13.15) (70.0.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from APScheduler==3.6.3->python-telegram-bot==13.15) (1.16.0)\nCollecting tzlocal>=1.2 (from APScheduler==3.6.3->python-telegram-bot==13.15)\n  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\nDownloading python_telegram_bot-13.15-py3-none-any.whl (519 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading APScheduler-3.6.3-py2.py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cachetools-4.2.2-py3-none-any.whl (11 kB)\nDownloading tzlocal-5.2-py3-none-any.whl (17 kB)\nBuilding wheels for collected packages: tornado\n  Building wheel for tornado (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for tornado: filename=tornado-6.1-cp310-cp310-linux_x86_64.whl size=417223 sha256=ac873d3f012e33fb3a7293d8a9d5c054c0337984302bd3744060ac6ba1518a4f\n  Stored in directory: /root/.cache/pip/wheels/80/32/8d/21cf0fa6ee4e083f6530e5b83dfdfa9489a3890d320803f4c7\nSuccessfully built tornado\nInstalling collected packages: tzlocal, tornado, cachetools, APScheduler, python-telegram-bot\n  Attempting uninstall: tornado\n    Found existing installation: tornado 6.4.1\n    Uninstalling tornado-6.4.1:\n      Successfully uninstalled tornado-6.4.1\n  Attempting uninstall: cachetools\n    Found existing installation: cachetools 4.2.4\n    Uninstalling cachetools-4.2.4:\n      Successfully uninstalled cachetools-4.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.3 requires cubinlinker, which is not installed.\ncudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\nbokeh 3.5.2 requires tornado>=6.2, but you have tornado 6.1 which is incompatible.\ncudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires cachetools>=4.2.4, but you have cachetools 4.2.2 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\njupyter-client 7.4.9 requires tornado>=6.2, but you have tornado 6.1 which is incompatible.\njupyter-server 2.12.5 requires tornado>=6.2.0, but you have tornado 6.1 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab 4.2.5 requires tornado>=6.2.0, but you have tornado 6.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed APScheduler-3.6.3 cachetools-4.2.2 python-telegram-bot-13.15 tornado-6.1 tzlocal-5.2\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import transformers\nimport torch\nimport numpy as np\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import BitsAndBytesConfig\nfrom transformers import pipeline\nfrom huggingface_hub import login\nlogin(token=\"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\")\n\nimport logging\nfrom telegram import Update\nfrom telegram.ext import Updater, CommandHandler, MessageHandler, Filters, CallbackContext\nfrom transformers import pipeline\n\n# Enable logging\nlogging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-19T11:08:30.168273Z","iopub.execute_input":"2024-11-19T11:08:30.168663Z","iopub.status.idle":"2024-11-19T11:08:48.328091Z","shell.execute_reply.started":"2024-11-19T11:08:30.168631Z","shell.execute_reply":"2024-11-19T11:08:48.327365Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model_id = \"/kaggle/input/fine-tuned-model-llm-project/trained-model\"\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nconfig = PeftConfig.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n    return_dict=True,\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\ntokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = PeftModel.from_pretrained(model, model_id)\n\ngeneration_config = model.generation_config\ngeneration_config.max_new_tokens = 150\ngeneration_config.temperature = 0.5\ngeneration_config.top_p = 0.8\ngeneration_config.num_return_sequences = 1\ngeneration_config.pad_token_id = tokenizer.eos_token_id\ngeneration_config.eos_token_id = tokenizer.eos_token_id\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T11:08:48.329086Z","iopub.execute_input":"2024-11-19T11:08:48.329621Z","iopub.status.idle":"2024-11-19T11:10:06.600569Z","shell.execute_reply.started":"2024-11-19T11:08:48.329586Z","shell.execute_reply":"2024-11-19T11:10:06.599745Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a695489cf07a4e729871ee6018549a84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf11717824f74a5681a1b5a6221fa1db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827986c5cff044638e0b9f9a4588bec3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd41969bfc074c55a7a241ba1ebc0192"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2be1dbfd59f84d96a219549173ab8b3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"504f0dd28c594c8aa92f856f66bbb8be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c095ff6b6f84438c87df655ecbbc1a18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"905715fad4914a2e922a589da7eef449"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29f31382ae1a417d8bfa6cd6a478882b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e0490a72efb4fc3b69e67c3351a3ee1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d551a7e286534ae89034bcaabcb7a4a6"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# generator = pipeline(\n#     \"text-generation\",\n#     model=model,\n#     tokenizer=tokenizer,\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T11:10:06.603182Z","iopub.execute_input":"2024-11-19T11:10:06.604177Z","iopub.status.idle":"2024-11-19T11:10:06.607712Z","shell.execute_reply.started":"2024-11-19T11:10:06.604146Z","shell.execute_reply":"2024-11-19T11:10:06.606845Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def post_process(predicted_text):\n    index = predicted_text.find(\"[/INST]\")\n    if(index==-1):\n        return predicted_text\n    \n    predicted_text=predicted_text[index:].strip()\n    predicted_text=predicted_text[len(\"[/INST]\"):].strip()\n    return predicted_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T11:10:06.608704Z","iopub.execute_input":"2024-11-19T11:10:06.608999Z","iopub.status.idle":"2024-11-19T11:10:06.624851Z","shell.execute_reply.started":"2024-11-19T11:10:06.608974Z","shell.execute_reply":"2024-11-19T11:10:06.624023Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Define a start command\ndef start(update: Update, context: CallbackContext):\n    update.message.reply_text('Hi! How can I assist you?')\n\n# Handle messages and generate responses\ndef handle_message(update: Update, context: CallbackContext):\n    user_input = update.message.text\n    prompt=f\"<s> [INST] <<SYS>> You are a helpful assistant, who always provides empathetic responses to the user's query and helps them solve their mental problems. <</SYS>> {user_input} [/INST]\".strip()\n    device = \"cuda\"\n    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.inference_mode():\n        outputs = model.generate(\n            input_ids = encoding.input_ids,\n            attention_mask = encoding.attention_mask,\n            generation_config = generation_config\n        )\n\n    predicted_text=(tokenizer.decode(outputs[0], skip_special_tokens=True))\n    predicted_text=post_process(predicted_text)\n    \n    update.message.reply_text(predicted_text)\n\n# Handle errors\ndef error(update: Update, context: CallbackContext):\n    logger.warning(f'Update {update} caused error {context.error}')\n\n# Main function to start the bot\ndef main():\n    # Replace 'YOUR_TOKEN_HERE' with your actual Telegram bot token\n    updater = Updater(\"7749794866:AAEKBApShqf4I10fapKKnh2yliUDAZtQDvs\", use_context=True)\n    \n    dp = updater.dispatcher\n\n    # Register handlers\n    dp.add_handler(CommandHandler(\"start\", start))\n    dp.add_handler(MessageHandler(Filters.text & ~Filters.command, handle_message))\n\n    dp.add_error_handler(error)\n\n    # Start the Bot\n    updater.start_polling()\n    \n    updater.idle()\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T11:10:06.625857Z","iopub.execute_input":"2024-11-19T11:10:06.626152Z"}},"outputs":[],"execution_count":null}]}